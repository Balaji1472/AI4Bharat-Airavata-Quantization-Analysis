{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nXfuzmN4hyf",
    "outputId": "029e5095-84e6-4815-b5cb-07e7f49bfb10"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages\n",
    "!pip install -q transformers torch accelerate bitsandbytes\n",
    "!pip install -q datasets evaluate\n",
    "!pip install -q ctransformers\n",
    "!pip install -q gputil psutil\n",
    "!pip install -q huggingface_hub\n",
    "!pip install -q pandas matplotlib seaborn\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import transformers\n",
    "import time\n",
    "import psutil\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "import gc\n",
    "import os\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ld5ig8df4_Jf"
   },
   "outputs": [],
   "source": [
    "def get_model_size_mb(model):\n",
    "    \"\"\"Calculate model size in MB\"\"\"\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    # Assuming float16 (2 bytes per parameter)\n",
    "    size_mb = param_count * 2 / (1024**2)\n",
    "    return size_mb, param_count\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / (1024**2)  # MB\n",
    "        gpu_memory_cached = torch.cuda.memory_reserved() / (1024**2)  # MB\n",
    "    else:\n",
    "        gpu_memory = gpu_memory_cached = 0\n",
    "\n",
    "    cpu_memory = psutil.virtual_memory().used / (1024**2)  # MB\n",
    "\n",
    "    return {\n",
    "        'gpu_allocated_mb': gpu_memory,\n",
    "        'gpu_cached_mb': gpu_memory_cached,\n",
    "        'cpu_used_mb': cpu_memory\n",
    "    }\n",
    "\n",
    "def benchmark_model(model, tokenizer, test_prompts, model_name=\"Unknown\", device=\"auto\"):\n",
    "    \"\"\"Comprehensive benchmarking function\"\"\"\n",
    "    print(f\"\\nüîç Benchmarking {model_name}...\")\n",
    "\n",
    "    # Initialize results\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'model_size_mb': 0,\n",
    "        'param_count': 0,\n",
    "        'avg_latency_ms': 0,\n",
    "        'tokens_per_second': 0,\n",
    "        'memory_before_mb': 0,\n",
    "        'memory_after_mb': 0,\n",
    "        'responses': [],\n",
    "        'individual_times': []\n",
    "    }\n",
    "\n",
    "    # Get model size\n",
    "    size_mb, param_count = get_model_size_mb(model)\n",
    "    results['model_size_mb'] = size_mb\n",
    "    results['param_count'] = param_count\n",
    "\n",
    "    # Record memory before inference\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    memory_before = get_memory_usage()\n",
    "    results['memory_before_mb'] = memory_before['gpu_allocated_mb']\n",
    "\n",
    "    # Test inference\n",
    "    total_time = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    print(f\"Testing with {len(test_prompts)} prompts...\")\n",
    "\n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"  Processing prompt {i+1}/{len(test_prompts)}\")\n",
    "\n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "            # Generate response with timing\n",
    "            start_time = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else tokenizer.pad_token_id,\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Decode response\n",
    "            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = full_response[len(prompt):].strip()  # Remove input prompt from response\n",
    "\n",
    "            # Calculate metrics\n",
    "            inference_time = end_time - start_time\n",
    "            new_tokens = len(outputs[0]) - len(inputs['input_ids'][0])\n",
    "\n",
    "            total_time += inference_time\n",
    "            total_tokens += new_tokens\n",
    "\n",
    "            results['responses'].append({\n",
    "                'prompt': prompt,\n",
    "                'response': response[:200] + \"...\" if len(response) > 200 else response,\n",
    "                'time_ms': inference_time * 1000,\n",
    "                'tokens': new_tokens,\n",
    "                'tokens_per_sec': new_tokens / inference_time if inference_time > 0 else 0\n",
    "            })\n",
    "\n",
    "            results['individual_times'].append(inference_time * 1000)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {i+1}: {str(e)}\")\n",
    "            results['responses'].append({\n",
    "                'prompt': prompt,\n",
    "                'response': f\"ERROR: {str(e)}\",\n",
    "                'time_ms': 0,\n",
    "                'tokens': 0,\n",
    "                'tokens_per_sec': 0\n",
    "            })\n",
    "\n",
    "    # Calculate averages\n",
    "    if total_time > 0:\n",
    "        results['avg_latency_ms'] = (total_time / len(test_prompts)) * 1000\n",
    "        results['tokens_per_second'] = total_tokens / total_time\n",
    "\n",
    "    # Record memory after inference\n",
    "    memory_after = get_memory_usage()\n",
    "    results['memory_after_mb'] = memory_after['gpu_allocated_mb']\n",
    "\n",
    "    print(f\"‚úÖ {model_name} benchmark complete!\")\n",
    "    print(f\"   Model size: {results['model_size_mb']:.1f} MB\")\n",
    "    print(f\"   Avg latency: {results['avg_latency_ms']:.1f} ms\")\n",
    "    print(f\"   Throughput: {results['tokens_per_second']:.1f} tok/s\")\n",
    "    print(f\"   GPU memory: {results['memory_after_mb']:.1f} MB\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ceBqQjeD7N_a",
    "outputId": "7cfd6247-a821-485d-e98e-5068db848934"
   },
   "outputs": [],
   "source": [
    "# Comprehensive test prompts in Hindi and English\n",
    "test_prompts = [\n",
    "    \"‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?\",\n",
    "    \"What is machine learning? Explain in simple terms.\",\n",
    "    \"‡§è‡§ï ‡§õ‡•ã‡§ü‡•Ä ‡§ï‡§π‡§æ‡§®‡•Ä ‡§≤‡§ø‡§ñ‡•á‡§Ç ‡§ú‡•ã ‡§™‡•ç‡§∞‡•á‡§∞‡§£‡§æ‡§¶‡§æ‡§Ø‡§ï ‡§π‡•ã‡•§\",\n",
    "    \"Write a Python function to calculate factorial.\",\n",
    "    \"‡§¶‡•Ä‡§™‡§æ‡§µ‡§≤‡•Ä ‡§ï‡§æ ‡§Æ‡§π‡§§‡•ç‡§µ ‡§¨‡§§‡§æ‡§á‡§è‡•§\",\n",
    "    \"Explain the difference between AI and ML.\",\n",
    "    \"‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§≠‡§æ‡§∑‡§æ ‡§ï‡§æ ‡§Æ‡§π‡§§‡•ç‡§µ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?\",\n",
    "    \"How does a neural network work?\",\n",
    "    \"‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø ‡§ï‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ‡§è‡§Ç ‡§¨‡§§‡§æ‡§è‡§Ç‡•§\",\n",
    "    \"What are the benefits of renewable energy?\"\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(test_prompts)} test prompts for evaluation\")\n",
    "print(\"\\nSample prompts:\")\n",
    "for i, prompt in enumerate(test_prompts[:5]):\n",
    "    print(f\"{i+1}. {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592,
     "referenced_widgets": [
      "edf98f9637874a86abbe68b874f62f80",
      "8297b4c0c151445993bf7b87b585ba51",
      "0b10143552ca48f3a67a155f154a6c9a",
      "30e0c651a473447293e9917ef49bafd6",
      "a17b7223faf14479b71cf4c5ee6ab42f",
      "d3c4d2d47eba4f669fa98f4862bcdbc5",
      "ddc261d02190474db7e349d39d05b60f",
      "890063886dcf4bb388922b3cd2efc348",
      "afaf1b9b9fe848d3a4bf29e2eade9bda",
      "f9ca6e038e5e46968e79de0575c52cd2",
      "012f5a2bfe6144d08454cfcc8a9943d8"
     ]
    },
    "id": "zzQVPJ6C7TCK",
    "outputId": "e9cee602-a056-4b3f-ad3b-9d1a4bc76407"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"üöÄ Loading original Airavata model...\")\n",
    "\n",
    "model_name = \"ai4bharat/Airavata\"\n",
    "\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    # Set pad token if not exists\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"‚úÖ Tokenizer loaded successfully\")\n",
    "\n",
    "    # Load model in float16 for memory efficiency\n",
    "    print(\"Loading original model (FP16)...\")\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Original model loaded successfully\")\n",
    "\n",
    "    # Run benchmark on original model\n",
    "    original_results = benchmark_model(\n",
    "        original_model,\n",
    "        tokenizer,\n",
    "        test_prompts,\n",
    "        \"Original (FP16)\"\n",
    "    )\n",
    "\n",
    "    # Clear GPU memory\n",
    "    del original_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"‚úÖ Original model benchmark complete and memory cleared\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading original model: {str(e)}\")\n",
    "    original_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhkrupJkEJaa",
    "outputId": "47c962d9-3b6b-49dd-aee9-32a3f34d66ce"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484,
     "referenced_widgets": [
      "7fd39a2f21e244c1a93dcb9a657292df",
      "125a900695ba406c946e745bbfc90c6f",
      "e3ca53444b164465b33ed4a7ddd124ed",
      "dad6557ffcef42bb9a56415cec74c9d1",
      "e55dd8e61947404abeb608777c3551f0",
      "39a66820e5df471eb0d63c67e7ce3070",
      "4f252c16b71b46c5806e39d958219f43",
      "8a669c7449f24d77b92391a465f7442d",
      "8335df1310ce4d138208d13cdfa5dcd4",
      "ece2748d8f034a02a5ceeb2c04fd040d",
      "29f1bc4bc5954e0fa2c94e5ecf8e9cfb"
     ]
    },
    "id": "r9KxUaMC8p47",
    "outputId": "f4241c83-9882-4c31-9ca5-4da42aeea41a"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüîß Testing 8-bit quantization...\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Load 8-bit quantized model\n",
    "    model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ 8-bit model loaded successfully\")\n",
    "\n",
    "    # Run benchmark\n",
    "    results_8bit = benchmark_model(\n",
    "        model_8bit,\n",
    "        tokenizer,\n",
    "        test_prompts,\n",
    "        \"8-bit Quantized\"\n",
    "    )\n",
    "\n",
    "    # Clear memory\n",
    "    del model_8bit\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"‚úÖ 8-bit model benchmark complete and memory cleared\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with 8-bit quantization: {str(e)}\")\n",
    "    results_8bit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrFqovIzE30K",
    "outputId": "7c70f665-33fd-48e1-c7bb-7c09f2d85dfc"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446,
     "referenced_widgets": [
      "aa46c6e4178b43d29560a968a706c5f3",
      "8edb12b6d84948989f43abb6a099b689",
      "5f89b860a88046a29b41b1437a15d401",
      "e3031891a2a8443e88b13ade551378c2",
      "0d3e655b6bce4a10928e558e14544aef",
      "6b5019fead3c4ec9a8e7574a59af5713",
      "4e44e249894c472285cfb51cbe4bb4f5",
      "8424bc83dccf4e38ae3a9f2ccfb7df1d",
      "3fbd328a4ecd468296e7e65c8b3afcf6",
      "95d80a18827b48639df2ab66b6934b9e",
      "e7cd0b56306243a58f365d349422fc72"
     ]
    },
    "id": "1PuOIR9p-BMM",
    "outputId": "cdf64b74-2ef6-4251-c7ad-158d746994ac"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüîß Testing 4-bit quantization...\")\n",
    "\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "\n",
    "    # Configure 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "    # Load 4-bit quantized model\n",
    "    model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ 4-bit model loaded successfully\")\n",
    "\n",
    "    # Run benchmark\n",
    "    results_4bit = benchmark_model(\n",
    "        model_4bit,\n",
    "        tokenizer,\n",
    "        test_prompts,\n",
    "        \"4-bit Quantized\"\n",
    "    )\n",
    "\n",
    "    # Clear memory\n",
    "    del model_4bit\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"‚úÖ 4-bit model benchmark complete and memory cleared\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with 4-bit quantization: {str(e)}\")\n",
    "    results_4bit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ammWodHFXfS",
    "outputId": "f4ca0a94-4237-4dbc-b9e6-1e4422d01693"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423,
     "referenced_widgets": [
      "0929eb8b225142e19644d2c56475a78b",
      "a243a15b6178453390bdabebad8f514c",
      "9e17a10e853949cdb9c8d43a2c01092d",
      "dcac430d0c5c4ae7a035267106ee32c1",
      "7134262a65ca4d8e8e0b4543843d817a",
      "c9f224aa78d548beb2f00f2133d11aa4",
      "d8d7dd804fc149f1a74a37908b72973b",
      "ba958bddcd9b4d7194c6812dff915f53",
      "57311931b0eb459f96cb29ba70956f5c",
      "e854f81bc7314b2392443eea1ca95732",
      "10c10fd9ec064519b025961665d6f8e7",
      "8681871358be4a7eb6d40b0fd120adaa",
      "414ea009823443028ad55f7f272debe3",
      "0a4b24634df04eccb760887ff62413b4",
      "091c6a678dfb44b59ee477df2512eaf3",
      "a0b8a7073bcd4b04b8bc4ce038b44ef0",
      "9ec48acdad974cb9abd250e0b0a2af32",
      "51b32c826e67417c98c56fc6f693a8a3",
      "b83f1d22b2264a1890b15ca2adfa6367",
      "0736530466254c038000bdeb28e718de",
      "f415b2d2a1294b5f9f6d28a4d32503e7",
      "b8b793748ce44886bfb4d6bb8ebf6752"
     ]
    },
    "id": "G7xQvg5E-mrG",
    "outputId": "96ea3ff4-c1fa-41fb-b473-6c71a50087ce"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüîß Testing GGUF quantized model...\")\n",
    "\n",
    "try:\n",
    "    from ctransformers import AutoModelForCausalLM as CTAutoModel\n",
    "    import os\n",
    "\n",
    "    # Try to load GGUF model\n",
    "    gguf_model = CTAutoModel.from_pretrained(\n",
    "        \"sam749/Airavata-GGUF\",\n",
    "        model_type=\"llama\",\n",
    "        gpu_layers=40  # Use GPU acceleration if available\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ GGUF model loaded successfully\")\n",
    "\n",
    "    # Custom benchmark function for GGUF\n",
    "    def benchmark_gguf_model(model, prompts, model_name):\n",
    "        print(f\"\\nüîç Benchmarking {model_name}...\")\n",
    "\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'model_size_mb': 0,  # Placeholder\n",
    "            'param_count': 0,    # Placeholder\n",
    "            'responses': [],\n",
    "            'individual_times': [],\n",
    "            'total_time': 0\n",
    "        }\n",
    "\n",
    "        total_time = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(f\"  Processing prompt {i+1}/{len(prompts)}\")\n",
    "\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                response = model(prompt, max_new_tokens=100, temperature=0.7)\n",
    "                end_time = time.time()\n",
    "\n",
    "                inference_time = end_time - start_time\n",
    "                total_time += inference_time\n",
    "\n",
    "                estimated_tokens = len(response.split()) * 1.3\n",
    "                total_tokens += estimated_tokens\n",
    "\n",
    "                results['responses'].append({\n",
    "                    'prompt': prompt,\n",
    "                    'response': response[:200] + \"...\" if len(response) > 200 else response,\n",
    "                    'time_ms': inference_time * 1000,\n",
    "                    'tokens': estimated_tokens,\n",
    "                    'tokens_per_sec': estimated_tokens / inference_time if inference_time > 0 else 0\n",
    "                })\n",
    "\n",
    "                results['individual_times'].append(inference_time * 1000)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing prompt {i+1}: {str(e)}\")\n",
    "                results['responses'].append({\n",
    "                    'prompt': prompt,\n",
    "                    'response': f\"ERROR: {str(e)}\",\n",
    "                    'time_ms': 0,\n",
    "                    'tokens': 0,\n",
    "                    'tokens_per_sec': 0\n",
    "                })\n",
    "\n",
    "        # Calculate averages\n",
    "        results['total_time'] = total_time\n",
    "        if total_time > 0 and len(prompts) > 0:\n",
    "            results['avg_latency_ms'] = (total_time / len(prompts)) * 1000\n",
    "            results['tokens_per_second'] = total_tokens / total_time\n",
    "        else:\n",
    "            results['avg_latency_ms'] = 0\n",
    "            results['tokens_per_second'] = 0\n",
    "\n",
    "        print(f\"‚úÖ {model_name} benchmark complete!\")\n",
    "        print(f\"   Avg latency: {results['avg_latency_ms']:.1f} ms\")\n",
    "        print(f\"   Throughput: {results['tokens_per_second']:.1f} tok/s\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Run GGUF benchmark\n",
    "    results_gguf = benchmark_gguf_model(gguf_model, test_prompts, \"GGUF Quantized\")\n",
    "\n",
    "    # üîß Inject model size manually (from download or disk)\n",
    "    # If downloaded via Hugging Face, you'll find path in huggingface cache or know the local .gguf path\n",
    "    try:\n",
    "        gguf_file_path = os.path.expanduser(\"~/.cache/huggingface/hub/models--sam749--Airavata-GGUF/snapshots\")  # Adjust if needed\n",
    "        for root, _, files in os.walk(gguf_file_path):\n",
    "            for f in files:\n",
    "                if f.endswith(\".gguf\") or f.endswith(\".bin\"):\n",
    "                    full_path = os.path.join(root, f)\n",
    "                    size_mb = os.path.getsize(full_path) / (1024 ** 2)\n",
    "                    results_gguf['model_size_mb'] = round(size_mb, 1)\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not determine GGUF model size: {e}\")\n",
    "        results_gguf['model_size_mb'] = 3700  # fallback estimate in MB\n",
    "\n",
    "    # üîß Set estimated parameter count (use 4-bit version as proxy)\n",
    "    results_gguf['param_count'] = 3632017408\n",
    "\n",
    "    # Clean up\n",
    "    del gguf_model\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"‚úÖ GGUF model benchmark complete and memory cleared\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with GGUF model: {str(e)}\")\n",
    "    results_gguf = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydYAFrDSER9G",
    "outputId": "aa545772-69a0-4b66-9a5d-49ed67650e2e"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüìä Analyzing and comparing results...\")\n",
    "\n",
    "# Compile all results\n",
    "all_results = {}\n",
    "if original_results:\n",
    "    all_results['Original (FP16)'] = original_results\n",
    "if results_8bit:\n",
    "    all_results['8-bit Quantized'] = results_8bit\n",
    "if results_4bit:\n",
    "    all_results['4-bit Quantized'] = results_4bit\n",
    "if results_gguf:\n",
    "    all_results['GGUF Quantized'] = results_gguf\n",
    "\n",
    "if not all_results:\n",
    "    print(\"‚ùå No successful benchmarks to analyze\")\n",
    "else:\n",
    "    print(f\"‚úÖ Successfully benchmarked {len(all_results)} model variants\")\n",
    "\n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for model_name, results in all_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Size (MB)': results.get('model_size_mb', 0),\n",
    "            'Parameters': results.get('param_count', 0),\n",
    "            'Avg Latency (ms)': results.get('avg_latency_ms', 0),\n",
    "            'Throughput (tok/s)': results.get('tokens_per_second', 0),\n",
    "            'GPU Memory (MB)': results.get('memory_after_mb', 0)\n",
    "        })\n",
    "\n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "    # Display comparison table\n",
    "    print(\"\\nüìã Performance Comparison Table:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df_comparison.to_string(index=False, float_format='%.1f'))\n",
    "\n",
    "    # Calculate compression ratios and performance improvements\n",
    "    if original_results:\n",
    "        original_size = original_results.get('model_size_mb', 1)\n",
    "        original_latency = original_results.get('avg_latency_ms', 1)\n",
    "        original_throughput = original_results.get('tokens_per_second', 1)\n",
    "\n",
    "        print(\"\\nüìà Compression and Performance Analysis:\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for model_name, results in all_results.items():\n",
    "            if model_name != 'Original (FP16)':\n",
    "                size_reduction = ((original_size - results.get('model_size_mb', 0)) / original_size) * 100\n",
    "                latency_change = ((results.get('avg_latency_ms', 0) - original_latency) / original_latency) * 100\n",
    "                throughput_change = ((results.get('tokens_per_second', 0) - original_throughput) / original_throughput) * 100\n",
    "\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                print(f\"  Size reduction: {size_reduction:.1f}%\")\n",
    "                print(f\"  Latency change: {latency_change:+.1f}%\")\n",
    "                print(f\"  Throughput change: {throughput_change:+.1f}%\")\n",
    "\n",
    "    # Save results to JSON\n",
    "    results_json = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'test_environment': {\n",
    "            'gpu_available': torch.cuda.is_available(),\n",
    "            'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'transformers_version': transformers.__version__\n",
    "        },\n",
    "        'test_prompts': test_prompts,\n",
    "        'results': all_results\n",
    "    }\n",
    "\n",
    "    # Save to file\n",
    "    with open('benchmark_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nüíæ Results saved to 'benchmark_results.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945
    },
    "id": "IsBLIhmBHLfl",
    "outputId": "41522cc2-2925-47eb-df2f-60bd6e184244"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating visualizations...\")\n",
    "\n",
    "if len(all_results) > 1:\n",
    "\n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Create subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Airavata Model Quantization Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    models = list(all_results.keys())\n",
    "    sizes = [all_results[m].get('model_size_mb', 0) for m in models]\n",
    "    latencies = [all_results[m].get('avg_latency_ms', 0) for m in models]\n",
    "    throughputs = [all_results[m].get('tokens_per_second', 0) for m in models]\n",
    "    memory_usage = [all_results[m].get('memory_after_mb', 0) for m in models]\n",
    "\n",
    "    # Plot 1: Model Size Comparison\n",
    "    bars1 = ax1.bar(models, sizes, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    ax1.set_title('Model Size Comparison', fontweight='bold')\n",
    "    ax1.set_ylabel('Size (MB)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, size in zip(bars1, sizes):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{size:.0f}MB', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Plot 2: Latency Comparison\n",
    "    bars2 = ax2.bar(models, latencies, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    ax2.set_title('Average Latency Comparison', fontweight='bold')\n",
    "    ax2.set_ylabel('Latency (ms)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for bar, latency in zip(bars2, latencies):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{latency:.0f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Plot 3: Throughput Comparison\n",
    "    bars3 = ax3.bar(models, throughputs, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    ax3.set_title('Throughput Comparison', fontweight='bold')\n",
    "    ax3.set_ylabel('Tokens per Second')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for bar, throughput in zip(bars3, throughputs):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{throughput:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Plot 4: Memory Usage Comparison\n",
    "    bars4 = ax4.bar(models, memory_usage, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    ax4.set_title('GPU Memory Usage', fontweight='bold')\n",
    "    ax4.set_ylabel('Memory (MB)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for bar, memory in zip(bars4, memory_usage):\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{memory:.0f}MB', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('quantization_performance_charts.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Create efficiency plot (Size vs Performance)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create scatter plot with size vs throughput\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "    for i, model in enumerate(models):\n",
    "        plt.scatter(sizes[i], throughputs[i], s=200, c=colors[i], alpha=0.7,\n",
    "                   edgecolors='black', linewidth=2, label=model)\n",
    "        plt.annotate(model, (sizes[i], throughputs[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontweight='bold', fontsize=9)\n",
    "\n",
    "    plt.title('Model Efficiency: Size vs Throughput', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Model Size (MB)', fontweight='bold')\n",
    "    plt.ylabel('Throughput (tokens/sec)', fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('efficiency_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úÖ Visualizations created and saved\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough models to create meaningful comparisons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kECB2E9ZHfqY",
    "outputId": "3ee2f9e9-bd39-452a-d64e-8805fa4e1685"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüîç Analyzing Response Quality...\")\n",
    "\n",
    "def analyze_response_quality(all_results, sample_size=3):\n",
    "    \"\"\"Analyze and compare response quality across models\"\"\"\n",
    "\n",
    "    if not all_results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "\n",
    "    print(f\"Comparing first {sample_size} responses across all models:\\n\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Get sample responses for comparison\n",
    "    for i in range(min(sample_size, len(test_prompts))):\n",
    "        print(f\"\\nüìù PROMPT {i+1}: {test_prompts[i]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        for model_name, results in all_results.items():\n",
    "            if i < len(results['responses']):\n",
    "                response = results['responses'][i]['response']\n",
    "                time_ms = results['responses'][i]['time_ms']\n",
    "                print(f\"\\n{model_name} ({time_ms:.0f}ms):\")\n",
    "                print(f\"  {response}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "# Run quality analysis\n",
    "analyze_response_quality(all_results, sample_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6iFU0hBhHwak",
    "outputId": "32e46348-e741-4eb9-b4dc-88097ba86a87"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_comprehensive_report(all_results, test_prompts):\n",
    "    \"\"\"Generate a comprehensive markdown report\"\"\"\n",
    "\n",
    "    report = f\"\"\"# Airavata Model Quantization Performance Report\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Base Model**: ai4bharat/Airavata (7B parameters)\n",
    "**Test Environment**: Google Colab\n",
    "**GPU**: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\n",
    "**Test Prompts**: {len(test_prompts)} (Hindi and English mix)\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    if len(all_results) > 1:\n",
    "        # Find best performing models\n",
    "        best_speed = max(all_results.items(), key=lambda x: x[1].get('tokens_per_second', 0))\n",
    "        smallest_size = min(all_results.items(), key=lambda x: x[1].get('model_size_mb', float('inf')))\n",
    "\n",
    "        report += f\"\"\"\n",
    "### üèÜ Best Performers\n",
    "- **Fastest Model**: {best_speed[0]} ({best_speed[1].get('tokens_per_second', 0):.1f} tok/s)\n",
    "- **Smallest Model**: {smallest_size[0]} ({smallest_size[1].get('model_size_mb', 0):.1f} MB)\n",
    "\n",
    "### üìä Performance Summary\n",
    "\"\"\"\n",
    "\n",
    "        # Create performance table\n",
    "        report += \"\\n| Model | Size (MB) | Latency (ms) | Throughput (tok/s) | GPU Memory (MB) |\\n\"\n",
    "        report += \"|-------|-----------|--------------|-------------------|------------------|\\n\"\n",
    "\n",
    "        for model_name, results in all_results.items():\n",
    "            report += f\"| {model_name} | {results.get('model_size_mb', 0):.1f} | {results.get('avg_latency_ms', 0):.1f} | {results.get('tokens_per_second', 0):.1f} | {results.get('memory_after_mb', 0):.1f} |\\n\"\n",
    "\n",
    "    # Add detailed analysis\n",
    "    report += f\"\"\"\n",
    "\n",
    "## Detailed Analysis\n",
    "\n",
    "### Test Configuration\n",
    "- **Number of test prompts**: {len(test_prompts)}\n",
    "- **Max tokens per response**: 100\n",
    "- **Temperature**: 0.7\n",
    "- **Repetition penalty**: 1.1\n",
    "\n",
    "### Model Variants Tested\n",
    "\"\"\"\n",
    "\n",
    "    for model_name, results in all_results.items():\n",
    "        report += f\"\"\"\n",
    "#### {model_name}\n",
    "- **Model Size**: {results.get('model_size_mb', 0):.1f} MB\n",
    "- **Parameters**: {results.get('param_count', 'N/A')}\n",
    "- **Average Latency**: {results.get('avg_latency_ms', 0):.1f} ms\n",
    "- **Throughput**: {results.get('tokens_per_second', 0):.1f} tokens/sec\n",
    "- **GPU Memory**: {results.get('memory_after_mb', 0):.1f} MB\n",
    "\"\"\"\n",
    "\n",
    "    # Recommendations\n",
    "    report += \"\"\"\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "### For Production Deployment\n",
    "\"\"\"\n",
    "\n",
    "    if results_4bit:\n",
    "        report += \"\"\"\n",
    "**4-bit Quantization** appears to offer the best balance of:\n",
    "- Significant size reduction (~75%)\n",
    "- Good performance maintenance\n",
    "- Reasonable memory usage\n",
    "\"\"\"\n",
    "\n",
    "    if results_gguf:\n",
    "        report += \"\"\"\n",
    "**GGUF Format** is recommended for:\n",
    "- CPU-only inference\n",
    "- Edge deployment scenarios\n",
    "- Maximum compatibility\n",
    "\"\"\"\n",
    "\n",
    "    report += \"\"\"\n",
    "\n",
    "### Next Steps\n",
    "1. **Phase 2**: Implement FastAPI backend with selected quantization method\n",
    "2. **Phase 3**: Test on target hardware (CPU deployment)\n",
    "3. **Phase 4**: Performance optimization and final benchmarking\n",
    "\n",
    "## Technical Notes\n",
    "\n",
    "### Quantization Methods Tested\n",
    "- **8-bit**: Uses BitsAndBytes library for 8-bit integer quantization\n",
    "- **4-bit**: NF4 quantization with double quantization enabled\n",
    "- **GGUF**: CPU-optimized format with various quantization levels\n",
    "\n",
    "### Limitations\n",
    "- Tests performed in Google Colab environment\n",
    "- Results may vary on different hardware configurations\n",
    "- Quality assessment is preliminary (manual evaluation recommended)\n",
    "\n",
    "---\n",
    "\n",
    "*Report generated automatically by the Airavata Model Analysis pipeline*\n",
    "\"\"\"\n",
    "\n",
    "    return report\n",
    "\n",
    "# Generate and save the final report\n",
    "print(\"\\nüìã Generating comprehensive report...\")\n",
    "\n",
    "final_report = generate_comprehensive_report(all_results, test_prompts)\n",
    "\n",
    "# Save the report\n",
    "with open('airavata_quantization_report.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(final_report)\n",
    "\n",
    "print(\"‚úÖ Comprehensive report saved to 'airavata_quantization_report.md'\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ PHASE 1 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ Tested {len(all_results)} model variants\")\n",
    "print(f\"‚úÖ Processed {len(test_prompts)} test prompts\")\n",
    "print(\"‚úÖ Generated performance comparisons\")\n",
    "print(\"‚úÖ Created visualizations\")\n",
    "print(\"‚úÖ Saved detailed report\")\n",
    "\n",
    "if all_results:\n",
    "    print(f\"\\nüèÜ RECOMMENDED FOR PHASE 2:\")\n",
    "    # Simple recommendation logic\n",
    "    if results_gguf and results_gguf.get('tokens_per_second', 0) > 0:\n",
    "        print(\"   GGUF Format (best for CPU deployment)\")\n",
    "    elif results_4bit and results_4bit.get('tokens_per_second', 0) > 0:\n",
    "        print(\"   4-bit Quantization (best balance of size and performance)\")\n",
    "    elif results_8bit and results_8bit.get('tokens_per_second', 0) > 0:\n",
    "        print(\"   8-bit Quantization (good performance with moderate compression)\")\n",
    "    else:\n",
    "        print(\"   Review results to choose optimal quantization method\")\n",
    "\n",
    "print(\"\\nüìÅ FILES GENERATED:\")\n",
    "print(\"   - benchmark_results.json\")\n",
    "print(\"   - airavata_quantization_report.md\")\n",
    "print(\"   - quantization_performance_charts.png\")\n",
    "print(\"   - efficiency_analysis.png\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
