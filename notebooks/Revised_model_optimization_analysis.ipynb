{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xOdyvW0Q5RJ",
    "outputId": "f6865abd-1645-4892-abe4-91d976f0b9c3"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub ctransformers\n",
    "\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403,
     "referenced_widgets": [
      "817739d882cc4c2c8fc8d8d2362a0b5f",
      "99a979f05cdb469e81a517bc38daeecd",
      "b2648e83cda14a199c53ed48ef025ae3",
      "91e359f180bd4042b270fd67c73aafa5",
      "6da50c4119f541a2a34bd17047432139",
      "6a7edaf39f2242bca0d17278b0483c26",
      "8b847f764ab0461490cd52553077ed55",
      "d49847e9227b47ada8776837118a8215",
      "8565fae77dea4d74969b066ca69aae7d",
      "54d36e0a07ce45778d7eb1af8e3ba4c1",
      "bdc54cc0157c4630b590434cbe6f9a85",
      "5a88a4a3e2e34756b39cbce29040d065",
      "652c5f5840904568999e2d17b5895755",
      "44de5238812d4a83a9c83835ed910e26",
      "3628a06b68a143a6bde5676bd3fa7557",
      "c509220156004544abe1539c5615d74d",
      "c35db4c9c4ed49e5a97c8c2b0973730d",
      "171694cfed1d486a9dff52a0e8d4aa39",
      "ef70c2c5c16b4d088c9b532d811a12d2",
      "b67c025a190048149458434e6e887c0b",
      "b70bdc07883b4f05877aec8fc42a3d75",
      "da193a6ef6c14c8097996cb5c4164496",
      "52f2c5120dad41abbd48489cc6f8ed0c",
      "65bfbca771024c67af7fbcf14aa26712",
      "5751705f5acb4f92b961c4e2c298e852",
      "9ed30d452c774a2a8f5afee4764edbd4",
      "c423fd40460d4e9abc75556e004f4499",
      "0b7ead091f1743d8ae7c8b4419d0bc44",
      "ec17e4a4de2047c782f1f785ff0a362a",
      "beb4db26050949beafe4352cd3e80745",
      "d2686c113fd64eeb93a3247e6c7498af",
      "e0e08aa509634e64ab1295904aac7a39",
      "754905e7989045ff86e91734809a131e"
     ]
    },
    "id": "_g-srsBcRRx9",
    "outputId": "457129cc-0cbf-40f1-80f5-fb4ed6d9790c"
   },
   "outputs": [],
   "source": [
    "model_dir = \"models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "selected_models = [\n",
    "    \"Airavata.Q2_K.gguf\",\n",
    "    \"Airavata.Q4_K_M.gguf\",\n",
    "    \"Airavata.Q6_K.gguf\"\n",
    "]\n",
    "\n",
    "for model_file in selected_models:\n",
    "    print(f\"Downloading {model_file} ...\")\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"mradermacher/Airavata-GGUF\",\n",
    "        filename=model_file,\n",
    "        local_dir=model_dir,\n",
    "        local_dir_use_symlinks=False\n",
    "    )\n",
    "    print(f\"Downloaded: {model_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ilc3Vfj7RR0o",
    "outputId": "764144c4-20ef-41bd-8dc5-eb6079b71e82"
   },
   "outputs": [],
   "source": [
    "!pip install ctransformers pandas matplotlib seaborn huggingface_hub\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAf9ORzmRR3P"
   },
   "outputs": [],
   "source": [
    "def benchmark_model(model, prompt, max_tokens=100):\n",
    "    start_time = time.time()\n",
    "    first_token_time = None\n",
    "    response = \"\"\n",
    "\n",
    "    try:\n",
    "        for token in model(prompt, max_new_tokens=max_tokens, stream=True):\n",
    "            if first_token_time is None:\n",
    "                first_token_time = time.time() - start_time\n",
    "            response += token\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        if first_token_time is None:\n",
    "            first_token_time = total_time\n",
    "\n",
    "        token_count = len(response.split())\n",
    "        throughput = token_count / total_time if total_time > 0 else 0\n",
    "\n",
    "        return {\n",
    "            'time_to_first_token': first_token_time,\n",
    "            'total_latency': total_time,\n",
    "            'throughput': throughput,\n",
    "            'response': response[:200],\n",
    "            'token_count': token_count\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'time_to_first_token': 0,\n",
    "            'total_latency': 0,\n",
    "            'throughput': 0,\n",
    "            'response': f\"Error: {str(e)}\",\n",
    "            'token_count': 0\n",
    "        }\n",
    "\n",
    "def get_file_size_mb(filepath):\n",
    "    return os.path.getsize(filepath) / (1024 * 1024)\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ph-EMb0-RR55",
    "outputId": "b7a4c071-b49f-4540-9ce0-30f8fa7fb579"
   },
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"What is Artificial Intelligence?\",\n",
    "    \"Explain photosynthesis.\",\n",
    "    \"Explain the Benefits of solar energy?\",\n",
    "    \"How to make bread?\",\n",
    "    \"Define about the India?\",\n",
    "    \"कृत्रिम बुद्धिमत्ता क्या है?\",\n",
    "    \"प्रकाश संश्लेषण की व्याख्या करें।\",\n",
    "    \"सौर ऊर्जा के लाभों की व्याख्या करें।\",\n",
    "    \"ब्रेड कैसे बनाते हैं?\",\n",
    "    \"भारत के बारे में परिभाषित करें।\"\n",
    "]\n",
    "\n",
    "print(f\"Setup complete. {len(test_prompts)} test prompts ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VX2pIOUTRR8h",
    "outputId": "904dc80a-2cec-4a4e-ac25-509b2cc301e6"
   },
   "outputs": [],
   "source": [
    "model_path_q2k = \"models/Airavata.Q2_K.gguf\"\n",
    "results_q2k = []\n",
    "\n",
    "print(\"Loading Q2_K model...\")\n",
    "try:\n",
    "    model_q2k = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path_q2k,\n",
    "        model_type=\"llama\",\n",
    "        gpu_layers=10\n",
    "    )\n",
    "\n",
    "    file_size_q2k = get_file_size_mb(model_path_q2k)\n",
    "    print(f\"Q2_K model loaded. File size: {file_size_q2k:.2f} MB\")\n",
    "\n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"Testing Q2_K - Prompt {i+1}/10\")\n",
    "        result = benchmark_model(model_q2k, prompt)\n",
    "        result['model'] = 'Q2_K'\n",
    "        result['prompt_id'] = i+1\n",
    "        result['prompt'] = prompt\n",
    "        result['file_size_mb'] = file_size_q2k\n",
    "        results_q2k.append(result)\n",
    "\n",
    "    del model_q2k\n",
    "    clear_memory()\n",
    "    print(\"Q2_K benchmarking completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with Q2_K model: {str(e)}\")\n",
    "    results_q2k = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9oc8Oy1-RR_K",
    "outputId": "02f5d9a0-e555-46dc-c828-16944627ab7f"
   },
   "outputs": [],
   "source": [
    "model_path_q4km = \"models/Airavata.Q4_K_M.gguf\"\n",
    "results_q4km = []\n",
    "\n",
    "print(\"Loading Q4_K_M model...\")\n",
    "try:\n",
    "    model_q4km = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path_q4km,\n",
    "        model_type=\"llama\",\n",
    "        gpu_layers=10\n",
    "    )\n",
    "\n",
    "    file_size_q4km = get_file_size_mb(model_path_q4km)\n",
    "    print(f\"Q4_K_M model loaded. File size: {file_size_q4km:.2f} MB\")\n",
    "\n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"Testing Q4_K_M - Prompt {i+1}/10\")\n",
    "        result = benchmark_model(model_q4km, prompt)\n",
    "        result['model'] = 'Q4_K_M'\n",
    "        result['prompt_id'] = i+1\n",
    "        result['prompt'] = prompt\n",
    "        result['file_size_mb'] = file_size_q4km\n",
    "        results_q4km.append(result)\n",
    "\n",
    "    del model_q4km\n",
    "    clear_memory()\n",
    "    print(\"Q4_K_M benchmarking completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with Q4_K_M model: {str(e)}\")\n",
    "    results_q4km = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxjoVpFpRSCB",
    "outputId": "903071ed-bec2-4582-f5ba-99371816d947"
   },
   "outputs": [],
   "source": [
    "model_path_q6k = \"models/Airavata.Q6_K.gguf\"\n",
    "results_q6k = []\n",
    "\n",
    "print(\"Loading Q6_K model...\")\n",
    "try:\n",
    "    model_q6k = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path_q6k,\n",
    "        model_type=\"llama\",\n",
    "        gpu_layers=10\n",
    "    )\n",
    "\n",
    "    file_size_q6k = get_file_size_mb(model_path_q6k)\n",
    "    print(f\"Q6_K model loaded. File size: {file_size_q6k:.2f} MB\")\n",
    "\n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"Testing Q6_K - Prompt {i+1}/10\")\n",
    "        result = benchmark_model(model_q6k, prompt)\n",
    "        result['model'] = 'Q6_K'\n",
    "        result['prompt_id'] = i+1\n",
    "        result['prompt'] = prompt\n",
    "        result['file_size_mb'] = file_size_q6k\n",
    "        results_q6k.append(result)\n",
    "\n",
    "    del model_q6k\n",
    "    clear_memory()\n",
    "    print(\"Q6_K benchmarking completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with Q6_K model: {str(e)}\")\n",
    "    results_q6k = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yVdOzeaRSEw",
    "outputId": "72294dcf-f426-474a-ebef-720f24e35594"
   },
   "outputs": [],
   "source": [
    "all_results = results_q2k + results_q4km + results_q6k\n",
    "\n",
    "if all_results:\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "\n",
    "    # Select columns for CSV\n",
    "    csv_columns = ['model', 'time_to_first_token', 'total_latency', 'throughput', 'file_size_mb', 'prompt']\n",
    "    df_csv = df_results[csv_columns].copy()\n",
    "\n",
    "    print(\"Benchmark Results Summary:\")\n",
    "    print(df_results.groupby('model')[['time_to_first_token', 'total_latency', 'throughput', 'file_size_mb']].mean())\n",
    "\n",
    "    df_csv.to_csv('benchmark_results.csv', index=False,encoding='utf-8-sig')\n",
    "    print(\"Results saved to benchmark_results.csv\")\n",
    "else:\n",
    "    print(\"No results to compile. Check model loading errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "id": "s4L1VL_XRSHT",
    "outputId": "74b381cc-45ba-42c5-afee-d388674989e2"
   },
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Average metrics by model\n",
    "    model_summary = df_results.groupby('model').agg({\n",
    "        'time_to_first_token': 'mean',\n",
    "        'total_latency': 'mean',\n",
    "        'throughput': 'mean',\n",
    "        'file_size_mb': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Time to First Token\n",
    "    bars1 = axes[0,0].bar(model_summary['model'], model_summary['time_to_first_token'],\n",
    "                          color=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.8)\n",
    "    axes[0,0].set_title('Average Time to First Token', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Time (seconds)')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Total Latency\n",
    "    bars2 = axes[0,1].bar(model_summary['model'], model_summary['total_latency'],\n",
    "                          color=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.8)\n",
    "    axes[0,1].set_title('Average Total Latency', fontweight='bold')\n",
    "    axes[0,1].set_ylabel('Time (seconds)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Throughput\n",
    "    bars3 = axes[1,0].bar(model_summary['model'], model_summary['throughput'],\n",
    "                          color=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.8)\n",
    "    axes[1,0].set_title('Average Throughput', fontweight='bold')\n",
    "    axes[1,0].set_ylabel('Tokens/second')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    for bar in bars3:\n",
    "        height = bar.get_height()\n",
    "        axes[1,0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Size vs Performance\n",
    "    scatter = axes[1,1].scatter(model_summary['file_size_mb'], model_summary['throughput'],\n",
    "                                s=200, c=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.8)\n",
    "    for i, row in model_summary.iterrows():\n",
    "        axes[1,1].annotate(row['model'],\n",
    "                           (row['file_size_mb'], row['throughput']),\n",
    "                           xytext=(5, 5), textcoords='offset points',\n",
    "                           fontweight='bold')\n",
    "    axes[1,1].set_xlabel('Model Size (MB)')\n",
    "    axes[1,1].set_ylabel('Throughput (tokens/sec)')\n",
    "    axes[1,1].set_title('Model Size vs Performance Trade-off', fontweight='bold')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('professional_benchmark_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "8S4LXNdCZCIw",
    "outputId": "6b812fbd-84f0-4814-cf05-2871739c6877"
   },
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    df_results['language'] = df_results['prompt_id'].apply(lambda x: 'English' if x <= 5 else 'Hindi')\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Performance by Language', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Throughput by language\n",
    "    lang_throughput = df_results.groupby(['model', 'language'])['throughput'].mean().unstack()\n",
    "    lang_throughput.plot(kind='bar', ax=axes[0], color=['#3498db', '#e74c3c'], alpha=0.8)\n",
    "    axes[0].set_title('Average Throughput by Language', fontweight='bold')\n",
    "    axes[0].set_ylabel('Tokens/second')\n",
    "    axes[0].set_xlabel('Model')\n",
    "    axes[0].legend(title='Language', frameon=True)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Latency by language\n",
    "    lang_latency = df_results.groupby(['model', 'language'])['total_latency'].mean().unstack()\n",
    "    lang_latency.plot(kind='bar', ax=axes[1], color=['#3498db', '#e74c3c'], alpha=0.8)\n",
    "    axes[1].set_title('Average Latency by Language', fontweight='bold')\n",
    "    axes[1].set_ylabel('Latency (seconds)')\n",
    "    axes[1].set_xlabel('Model')\n",
    "    axes[1].legend(title='Language', frameon=True)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('language_performance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L2stDpJhRSMv",
    "outputId": "b61fbd0d-edc8-4bb3-b2e0-2c01f3db971d"
   },
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    model_summary = df_results.groupby('model').agg({\n",
    "        'time_to_first_token': ['mean', 'std'],\n",
    "        'total_latency': ['mean', 'std'],\n",
    "        'throughput': ['mean', 'std'],\n",
    "        'file_size_mb': 'first'\n",
    "    }).round(4)\n",
    "\n",
    "    model_summary.columns = ['_'.join(col).strip() for col in model_summary.columns]\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"FINAL BENCHMARK REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nDETAILED PERFORMANCE METRICS:\")\n",
    "    print(model_summary)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERFORMANCE RANKING:\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Rank models by different criteria\n",
    "    avg_metrics = df_results.groupby('model')[['time_to_first_token', 'total_latency', 'throughput', 'file_size_mb']].mean()\n",
    "\n",
    "    print(f\"\\n1. FASTEST FIRST TOKEN: {avg_metrics['time_to_first_token'].idxmin()} ({avg_metrics['time_to_first_token'].min():.3f}s)\")\n",
    "    print(f\"2. LOWEST LATENCY: {avg_metrics['total_latency'].idxmin()} ({avg_metrics['total_latency'].min():.2f}s)\")\n",
    "    print(f\"3. HIGHEST THROUGHPUT: {avg_metrics['throughput'].idxmax()} ({avg_metrics['throughput'].max():.1f} tokens/s)\")\n",
    "    print(f\"4. SMALLEST SIZE: {avg_metrics['file_size_mb'].idxmin()} ({avg_metrics['file_size_mb'].min():.0f} MB)\")\n",
    "\n",
    "    # Calculate efficiency score (throughput per MB)\n",
    "    avg_metrics['efficiency'] = avg_metrics['throughput'] / avg_metrics['file_size_mb']\n",
    "    best_efficiency = avg_metrics['efficiency'].idxmax()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATION:\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    best_overall = avg_metrics['throughput'].idxmax()\n",
    "\n",
    "    print(f\"\\nBEST OVERALL MODEL: {best_overall}\")\n",
    "    print(f\"- Throughput: {avg_metrics.loc[best_overall, 'throughput']:.1f} tokens/s\")\n",
    "    print(f\"- Latency: {avg_metrics.loc[best_overall, 'total_latency']:.2f}s\")\n",
    "    print(f\"- First Token: {avg_metrics.loc[best_overall, 'time_to_first_token']:.3f}s\")\n",
    "    print(f\"- Size: {avg_metrics.loc[best_overall, 'file_size_mb']:.0f} MB\")\n",
    "\n",
    "    print(f\"\\nMOST EFFICIENT MODEL: {best_efficiency}\")\n",
    "    print(f\"- Efficiency Score: {avg_metrics.loc[best_efficiency, 'efficiency']:.3f} tokens/s/MB\")\n",
    "\n",
    "    print(f\"\\nSMALLEST MODEL: {avg_metrics['file_size_mb'].idxmin()}\")\n",
    "    print(f\"- Good for resource-constrained environments\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"No results available for final report.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
